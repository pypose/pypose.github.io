<h1 id="issue-2">Issue #2</h1>

<p>Welcome to the second edition of the PyTorch newsletter! In this issue, read about how we celebrated the PyTorch community at the first-ever PyTorch Ecosystem Day (PTED), discover a new podcast for PyTorch developers, and learn about important updates to the PyTorch frontend.</p>

<h2 id="pytorch-ecosystem-day">PyTorch Ecosystem Day</h2>

<p><strong>Piotr Bialecki (Sr. Software Engineer, NVIDIA)</strong> spoke about his journey of using PyTorch and what he sees in the future for PyTorch. <strong>Miquel Farré (Sr. Technology Manager, Disney)</strong> spoke about the Creative Genome project that uses the PyTorch ecosystem to annotate all Disney content. <strong>Ritchie Ng (CEO, Hessian Matrix)</strong> spoke about the growth of AI in the Asia Pacific region, and how to get started with PyTorch for production AI use cases. Members of the community showcased how they were using PyTorch via 71 posters and pop-up breakout sessions. See all of the <a href="https://pytorch.org/ecosystem/pted/2021">posters</a> and listen to the opening <a href="https://www.youtube.com/playlist?list=PL_lsbAsL_o2At9NcX1mR9d12KYUWqxOx9">keynote talks</a> here!</p>

<h2 id="pytorch-developer-podcast">PyTorch Developer Podcast</h2>

<p><strong>Edward Yang (Research Engineer, Facebook AI)</strong> talks about internal development concepts like binding C++ in Python, the dispatcher, PyTorch’s library structure and more. Check out this new series; each episode is around 15 minutes long. <a href="https://pytorch-dev-podcast.simplecast.com/">Listen to it</a> wherever you get your podcasts.</p>

<h2 id="forward-mode-ad">Forward Mode AD</h2>
<p>The core logic for Forward Mode AD (based on “dual tensors”) is now in PyTorch. All the APIs to manipulate such Tensors, codegen and view handling are in <code class="language-plaintext highlighter-rouge">master (1.9.0a0)</code> already. Gradcheck and a first set of formulas will be added in the following month; full support for all PyTorch functions, custom Autograd functions and higher order gradients will happen later this year. Read more about this or share your feedback with <a href="https://github.com/albanD">@albanD</a> on the corresponding <a href="https://github.com/pytorch/rfcs/pull/11">RFC</a>.</p>

<h2 id="make-complex-conjugation-lazy">Make complex conjugation lazy</h2>

<p><a href="https://github.com/pytorch/pytorch/pull/54987">PR #54987</a> makes the conjugate operation on complex tensors return a view that has a special <code class="language-plaintext highlighter-rouge">is_conj()</code> bit flipped. Aside from saving memory by not creating a full tensor, this grants a potential speedup if the following operation can handle conjugated inputs directly. For such operations (like <code class="language-plaintext highlighter-rouge">gemm</code>), a flag is passed to the low-level API; for others the conjugate is materialized before passing to the operation.</p>

<h2 id="torchuse_deterministic_algorithms-is-stable">torch.use_deterministic_algorithms is stable</h2>

<p><code class="language-plaintext highlighter-rouge">torch.use_deterministic_algorithms()</code> (<a href="https://pytorch.org/docs/master/generated/torch.use_deterministic_algorithms.html">docs</a>) is stable in <code class="language-plaintext highlighter-rouge">master (1.9.0a0)</code>. If True, the flag switches non-deterministic operations to their deterministic implementation if available, and throws a <code class="language-plaintext highlighter-rouge">RuntimeError</code> if not.</p>

<h2 id="torchlinalg-and-torchspecial">torch.linalg and torch.special</h2>

<p><code class="language-plaintext highlighter-rouge">torch.linalg</code> is now stable; the module maintains fidelity with NumPy’s np.linalg linear algebra functions.
<code class="language-plaintext highlighter-rouge">torch.special</code> (beta) contains functions in scipy.special. Here’s the <a href="https://github.com/pytorch/pytorch/issues/50345">tracking issue</a> if you’d like to contribute functions to torch.special. If you want a function not already on the list, let us know on the tracking issue about your use case and why it should be added.</p>

<h2 id="generalizing-amp-to-work-on-cpu">Generalizing AMP to work on CPU</h2>

<blockquote>
  <p><a href="https://dev-discuss.pytorch.org/t/generalizing-amp-to-work-on-cpu/201">@ezyang</a>: Intel is interested in bringing automatic mixed precision to CPU in <a href="https://github.com/pytorch/pytorch/issues/55374">[RFC] Extend Autocast to CPU/CUDA with BF16 data type · Issue #55374 · pytorch/pytorch ·</a> One big question is what the API for autocasting should be for CPU; should we provide a single, generalized API torch.autocast (keep in mind that CPU autocasting would be through bfloat16, while the existing GPU autocasting is via float16), or provide separate APIs for CPU/CUDA? If you have any thoughts or opinions on the subject, please chime in on the issue.</p>
</blockquote>

<p><br />
<br /></p>

<p>Are you enjoying reading this newsletter? What would you like to know more about? All feedback is welcome and appreciated! To share your suggestions, use this <a href="https://forms.gle/K75ELciLJxnabKKH9">form</a> or simply reply to this email.</p>
