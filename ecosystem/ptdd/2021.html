<!DOCTYPE html>
<html lang="en">
  <head>
  <!-- Google Tag Manager -->
  <!-- <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-52DXT37');</script> -->
  <!-- End Google Tag Manager -->
  <!-- Second tag include -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-G6ST8LE9P6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-G6ST8LE9P6');
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Developer's Day 2021 | PyPose
    
  </title>
  
  <meta property="og:title" content="PyTorch" />
  <meta
    name="description"
    property="og:description"
    content="An open source machine learning framework that accelerates the path from research prototyping to production deployment."
  />
  <meta
  property="og:image"
  content="https://pypose.org/assets/images/pytorch-logo.png"
  />
  <meta property="og:url" content="https://www.pytorch.org" />


<meta property="og:type" content="website" />
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  <!--  -->
    <!-- <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->
 -->
  <!--  -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
  <!-- Google Tag Manager for PyPose -->
  <!-- <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script> -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-G6ST8LE9P6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G6ST8LE9P6');
</script> -->

</head>

  <body class="ecosystem">
    <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-52DXT37"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="container-fluid header-holder ecosystem-header">
  <div class="container">
    

<div class="header-container">
  <a class="header-logo" href="https://pypose.org" aria-label="PyPose"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <!-- <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow" href="/ecosystem">
          Ecosystem
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/pted/2021">
            <span class="dropdown-title">Ecosystem Day - 2021</span>
            <p>See the posters presented at ecosystem day 2021</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptdd/2021">
            <span class="dropdown-title">Developer Day - 2021</span>
            <p>See the posters presented at developer day 2021</p>
          </a>
        </div>
      </div>
    </li> -->

    <!-- <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item ">
      <a href="/blog">Blog</a>
    </li> -->

    <li class="main-menu-item">
      <a href="https://pypose.org/tutorials/">Tutorials</a>
    </li>
    
    <!-- <li class="main-menu-item ">
      <a href="/docs">Docs</a>
    </li> -->

    <li class="main-menu-item" id="docs-main-menu-link">
      <a href="/docs">Docs</a>
    </li>

    <li class="main-menu-item ">
      <a href="/about-us">About Us</a>
    </li>

    

    <!-- <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="doc-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li> -->

    <!-- 

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li> -->

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pypose/pypose">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

  </div>
</div>


    <div class="main-background features-background"></div>

    <div class="jumbotron jumbotron-fluid contributor-jumbotron">
  <div class="contributor-jumbo-text container">
    <h1>PyTorch Developer Day</h1>
    <h1 class="lead">2021</h1>
  </div>
</div>

<div class="main-content-wrapper">
  <div class="main-content">
    <div class="container">
      <p class="lead">
        PYTORCH DEVELOPER DAY: DAY 2 INFORMATION
      </p>
      <p class="lead">
        Community Talks:
      </p>
      <ul class="lead">
        <li>
          <a href="https://www.youtube.com/watch?v=7yQ4FgtYvj8"
            >PyTorch YouTube</a
          >
        </li>
      </ul>
      <div class="input-group mb-3">
        <input
          type="text"
          id="pted-filter"
          placeholder="Filter..."
          class="form-control"
        />
      </div>
      <hr />
      <div class="row">
        <h1>Posters</h1>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F8-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F8.png">xaitk-saliency: Saliency built for analytics and autonomy applications</a>
              
            </h5>
            <h6 class="card-subtitle">Brian Hu, Paul Tunison, Elim Schenck, Roddy Collins, Anthony Hoogs</h6>
            <p class="card-text">Despite significant progress in the past few years, machine learning-based systems are still often viewed as “black boxes,” which lack the ability to explain their output decisions to human users. Explainable artificial intelligence (XAI) attempts to help end-users understand and appropriately trust machine learning-based systems. One commonly used technique involves saliency maps, which are a form of visual explanation that reveals what an algorithm pays attention to during its decision process. We introduce the xaitk-saliency python package, an open-source, explainable AI framework and toolkit for visual saliency algorithm interfaces and implementations, built for analytics and autonomy applications. The framework is modular and easily extendable, with support for several image understanding tasks, including image classification, image similarity, and object detection. We have also recently added support for the autonomy domain, by creating saliency maps for pixel-based deep reinforcement-learning agents in environments such as ATARI. Several example notebooks are included that demo the current capabilities of the toolkit. xaitk-saliency will be of broad interest to anyone who wants to deploy AI capabilities in operational settings and needs to validate, characterize and trust AI performance across a wide range of real-world conditions and application areas using saliency maps. To learn more, please visit: https://github.com/XAITK/xaitk-saliency.</p>
            
            <p class="card-text">
              <a href="https://github.com/XAITK/xaitk-saliency">https://github.com/XAITK/xaitk-saliency</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">MEDICAL & HEALTHCARE, RESPONSIBLE AI</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F6-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F6.png">CovRNN—A collection of recurrent neural network models for predicting outcomes of COVID-19 patients using their EHR data</a>
              
            </h5>
            <h6 class="card-subtitle">Laila Rasmy, Ziqian Xie, Bingyu Mao, Khush Patel, Wanheng Zhang, Degui Zhi</h6>
            <p class="card-text">CovRNN is a collection of recurrent neural network (RNN)-based models to predict COVID-19 patients' outcomes, using their available electronic health record (EHR) data on admission, without the need for specific feature selection or missing data imputation. CovRNN is designed to predict three outcomes: in-hospital mortality, need for mechanical ventilation, and long length of stay (LOS >7 days). Predictions are made for time-to-event risk scores (survival prediction) and all-time risk scores (binary prediction). Our models were trained and validated using heterogeneous and de-identified data of 247,960 COVID-19 patients from 87 healthcare systems, derived from the Cerner® Real-World Dataset (CRWD) and 36,140 de-identified patients' data derived from the Optum® de-identified COVID-19 Electronic Health Record v. 1015 dataset (2007 - 2020). CovRNN shows higher performance than do traditional models. It achieved an area under the receiving operating characteristic (AUROC) of 93% for mortality and mechanical ventilation predictions on the CRWD test set (vs. 91·5% and 90% for light gradient boost machine (LGBM) and logistic regression (LR), respectively) and 86.5% for prediction of LOS > 7 days (vs. 81·7% and 80% for LGBM and LR, respectively). For survival prediction, CovRNN achieved a C-index of 86% for mortality and 92·6% for mechanical ventilation. External validation confirmed AUROCs in similar ranges. https://www.medrxiv.org/content/10.1101/2021.09.27.2126</p>
            
            <p class="card-text">
              <a href="https://github.com/ZhiGroup/CovRNN">https://github.com/ZhiGroup/CovRNN</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">MEDICAL & HEALTHCARE, RESPONSIBLE AI</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F5-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F5.png">Farabio - Deep learning for Biomedical Imaging</a>
              
            </h5>
            <h6 class="card-subtitle">Sanzhar Askaruly, Nurbolat Aimakov, Alisher Iskakov, Hyewon Cho, Yujin Ahn, Myeong Hoon Choi, Hyunmo Yang, Woonggyu Jung</h6>
            <p class="card-text">Deep learning has transformed many aspects of industrial pipelines recently. Scientists involved in biomedical imaging research are also benefiting from the power of AI to tackle complex challenges. Although the academic community has widely accepted image processing tools, such as scikit-image, ImageJ, there is still a need for a tool which integrates deep learning into biomedical image analysis. We propose a minimal, but convenient Python package based on PyTorch with common deep learning models, extended by flexible trainers and medical datasets. In this work, we also share theoretical dive in the form of course as well as minimal tutorials to run Android applications, containing models trained with Farabio.</p>
            
            <p class="card-text">
              <a href="https://github.com/tuttelikz/farabio">https://github.com/tuttelikz/farabio</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">MEDICAL & HEALTHCARE, RESPONSIBLE AI</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F4-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F4.png">TorchIO: Pre-processing & Augmentation of Medical Images for Deep Learning Applications</a>
              
            </h5>
            <h6 class="card-subtitle">Fernando Pérez-García, Rachel Sparks, Sébastien Ourselin</h6>
            <p class="card-text">Processing of medical images such as MRI or CT presents different challenges compared to RGB images typically used in computer vision: a lack of labels for large datasets, high computational costs, and the need of metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment and orientation of volumes. We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be easily composed, reproduced, traced and extended. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages good open-science practices, as it supports experiment reproducibility and is version-controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.</p>
            
            <p class="card-text">
              <a href="https://github.com/fepegar/torchio/">https://github.com/fepegar/torchio/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">MEDICAL & HEALTHCARE, RESPONSIBLE AI</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F3-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F3.png">MONAI: A Domain Specialized Library for Healthcare Imaging</a>
              
            </h5>
            <h6 class="card-subtitle">Michael Zephyr, Prerna Dogra, Richard Brown, Wenqi Li, Eric Kerfoot</h6>
            <p class="card-text">Healthcare image analysis for both radiology and pathology is increasingly being addressed with deep-learning-based solutions. These applications have specific requirements to support various imaging modalities like MR, CT, ultrasound, digital pathology, etc. It is a substantial effort for researchers in the field to develop custom functionalities to handle these requirements. Consequently, there has been duplication of effort, and as a result, researchers have incompatible tools, which makes it hard to collaborate. MONAI stands for Medical Open Network for AI. Its mission is to accelerate the development of healthcare imaging solutions by providing domain-specialized building blocks and a common foundation for the community to converge in a native PyTorch paradigm.</p>
            
            <p class="card-text">
              <a href="https://monai.io/">https://monai.io/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">MEDICAL & HEALTHCARE, RESPONSIBLE AI</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F2-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/F2.png">A Framework for Bayesian Neural Networks</a>
              
            </h5>
            <h6 class="card-subtitle">Sahar Karimi, Beliz Gokkaya, Audrey Flower, Ehsan Emamjomeh-Zadeh, Adly Templeton, Ilknur Kaynar Kabul, Erik Meijer</h6>
            <p class="card-text">We are presenting a framework for building Bayesian Neural Networks (BNN). One of the critical use cases of BNNs is uncertainty quantification of ML predictions in deep learning models. Uncertainty quantification leads to more robust and reliable ML systems that are often employed to prevent catastrophic outcomes of overconfident predictions especially in sensitive applications such as integrity, medical imaging and treatments, self driving cars, etc.. Our framework provides tools to build BNN models, estimate the uncertainty of their predictions, and transform existing models into their BNN counterparts. We discuss the building blocks and API of our framework along with a few examples and future directions.</p>
            
            <p class="card-text">
              <small class="text-muted">MEDICAL & HEALTHCARE, RESPONSIBLE AI</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E9-thumb.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E9-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E9-thumb.png">Revamp of torchvision datasets and transforms</a>
              
            </h5>
            <h6 class="card-subtitle">Philip Meier, torchvision team, torchdata team</h6>
            <p class="card-text">torchvision provides a lot of image and video datasets as well as transformations for research and prototyping. In fact, the very first release of torchvision in 2016 was all about these two submodules. Since their inception their extent has grown organically and became hard to maintain and sometimes also hard to use. Over the years we have gathered a lot of user feedback and decided to revamp the datasets and transforms. This poster will showcase the current state of the rework and compare it to the hopefully soon to be legacy API.</p>
            
            <p class="card-text">
              <a href="https://pytorchvideo.org/">https://pytorchvideo.org/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">AUDIO, IMAGE & VIDEO, VISION</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E8-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E8.png">OpenMMLab: Open-Source Toolboxes for Artificial Intelligence</a>
              
            </h5>
            <h6 class="card-subtitle">Wenwei Zhang, Han Lyu, Kai Chen</h6>
            <p class="card-text">OpenMMLab builds open-source tool boxes for computer vision. It aims to 1) provide high-quality codebases to reduce the difficulties in algorithm reimplementation; 2) create efficient deployment toolchains targeting a variety of inference engines and devices; 3) build a solid foundation for the community to bridge the gap between academic research and industrial applications. Based on PyTorch, OpenMMLab develops MMCV to provide unified abstract interfaces and common utils, which serve as a foundation of the whole system. Since the initial release in October 2018, OpenMMLab has released 15+ tool boxes covering different research areas. It has implemented 200+ algorithms and released contain 1800+ pre-trained models. With tighter collaboration with the community, OpenMMLab will open source more toolboxes and full-stack toolchains in the future.</p>
            
            <p class="card-text">
              <a href="openmmlab.com">openmmlab.com</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">AUDIO, IMAGE & VIDEO, VISION</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E5-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E5.png">Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning</a>
              
            </h5>
            <h6 class="card-subtitle">Siddha Ganju, Sayak Paul</h6>
            <p class="card-text">Floods wreak havoc throughout the world, causing billions of dollars in damages, and uprooting communities, ecosystems and economies. Aligning flood extent mapping with local topography can provide a plan-of-action that the disaster response team can consider. Thus, remote flood level estimation via satellites like Sentinel-1 can prove to be remedial. The Emerging Techniques in Computational Intelligence (ETCI) competition on Flood Detection tasked participants with predicting flooded pixels after training with synthetic aperture radar (SAR) images in a supervised setting. We use a cyclical approach involving two stages (1) training an ensemble model of multiple UNet architectures with available high and low confidence labeled data and, generating pseudo labels or low confidence labels on the entire unlabeled test dataset, and then, (2) filter out quality generated labels and, (3) combining the generated labels with the previously available high confidence labeled dataset. This assimilated dataset is used for the next round of training ensemble models. This cyclical process is repeated until the performance improvement plateaus. Additionally, we post-process our results with Conditional Random Fields. Our approach sets the second-highest score on the public hold-out test leaderboard for the ETCI competition with 0.7654 IoU. To the best of our knowledge we believe this is one of the first works to try out semi-supervised learning to improve flood segmentation models.</p>
            
            <p class="card-text">
              <a href="https://github.com/sidgan/ETCI-2021-Competition-on-FLood-Detection">https://github.com/sidgan/ETCI-2021-Competition-on-FLood-Detection</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">AUDIO, IMAGE & VIDEO, VISION</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E3-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E3.png">Real time Speech Enhancement</a>
              
            </h5>
            <h6 class="card-subtitle">Xiaoyu Liu, James Wagner, Roy Fejgin, Joan Serra, Santiago Pascual, Cong Zhou, Jordi Pons, Vivek Kumar</h6>
            <p class="card-text">Speech enhancement is a fundamental audio processing task that has experienced a radical change with the advent of deep learning technologies. We will overview the main characteristics of the task and the key principles of existing deep learning solutions. We will be presenting the past and present work done by our group with the overall goal of delivering the best possible intelligibility and sound quality. Finally, we will provide our view on the future of speech enhancement and show how our current long-term research aligns with such a view.</p>
            
            <p class="card-text">
              <small class="text-muted">AUDIO, IMAGE & VIDEO, VISION</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E2-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E2.png">Kornia AI: Low Level Computer Vision for AI</a>
              
            </h5>
            <h6 class="card-subtitle">Edgar Riba, Dmytro Mishkin, Jian Shi, Luis Ferraz</h6>
            <p class="card-text">Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models. It consists of a set of routines and differentiable modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions.</p>
            
            <p class="card-text">
              <a href="https://kornia.github.io//">https://kornia.github.io//</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">AUDIO, IMAGE & VIDEO, VISION</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E1-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/E1.png">Video Transformer Network</a>
              
            </h5>
            <h6 class="card-subtitle">Daniel Neimark, Omri Bar, Maya Zohar, Dotan Asselmann</h6>
            <p class="card-text">This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains 16.1× faster and runs 5.1× faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring 1.5× fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md . See paper: https://arxiv.org/abs/2102.00719</p>
            
            <p class="card-text">
              <a href="https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md">https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">AUDIO, IMAGE & VIDEO, VISION</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D7.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D7-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D7.png">DLRT: Ultra Low-Bit Precision Inference Engine for PyTorch on CPU</a>
              
            </h5>
            <h6 class="card-subtitle">Dr. Ehsan Saboori, Dr. Sudhakar Sah, MohammadHossein AskariHemmat Saad Ashfaq, Alex Hoffman, Olivier Mastropietro, Davis Sawyer</h6>
            <p class="card-text">The emergence of Deep Neural Networks (DNNs) on embedded and low-end devices holds tremendous potential to expand the adoption of AI technologies to wider audiences. However, making DNNs applicable for inference on such devices using techniques such as quantization and model compression, while maintaining model accuracy, remains a challenge for production deployment. Furthermore, there is a lack of inference engines available in any AI framework to run such low precision networks. Our work presents a novel inference engine and model compression framework that automatically enables PyTorch developers to quantize and run their deep learning models at 2bit and 1bit precision, making them faster, smaller and more energy-efficient in production. DLRT empowers PyTorch developers to unlock advanced AI on low-power CPUs, starting with ARM CPUs and MCUs. This work allows AI researchers and practitioners to achieve 10x faster inference and near-GPU level performance on a fraction of the power and cost.</p>
            
            <p class="card-text">
              <a href="https://github.com/deeplite">https://github.com/deeplite</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">PERFORMANCE, PRODUCTION & DEPLOYMENT</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D6-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D6.png">Serving PyTorch Models in Production at Walmart Search</a>
              
            </h5>
            <h6 class="card-subtitle">Adway Dhillo, Nidhin Pattaniyil</h6>
            <p class="card-text">This poster is for a data scientist or ML engineer looking to productionalize their pytorch models. It will cover post training steps that should be taken to optimize the model such as quantization and torch script. It will also walk the user in packaging and serving the model through Facebook’s TorchServe. Will also cover benefits of script mode and Pytorch JIT. Benefits of Torch Serve: high performance serving , multi model serving , model version for A/B testing, server side batching, support for pre and post processing</p>
            
            <p class="card-text">
              <a href="https://pytorch.org/serve/">https://pytorch.org/serve/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">PERFORMANCE, PRODUCTION & DEPLOYMENT</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D5-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D5.png">CleanRL: high-quality single file implementation of Deep Reinforcement Learning algorithms with research-friendly features</a>
              
            </h5>
            <h6 class="card-subtitle">Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga</h6>
            <p class="card-text">CleanRL is an open-source library that provides high-quality single-file implementations of Deep Reinforcement Learning algorithms. It provides a simpler yet scalable developing experience by having a straightforward codebase and integrating production tools to help interact and scale experiments. In CleanRL, we put all details of an algorithm into a single file, making these performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, videos of an agent's gameplay, dependencies, and more to the cloud. Despite succinct implementations, we have also designed tools to help scale, at one point orchestrating experiments on more than 2000 machines simultaneously via Docker and cloud providers.environments. The source code can be found at https://github.com/vwxyzjn/cleanrl.</p>
            
            <p class="card-text">
              <a href="https://github.com/vwxyzjn/cleanrl/">https://github.com/vwxyzjn/cleanrl/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">PERFORMANCE, PRODUCTION & DEPLOYMENT</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D4-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D4.png">Deploying a Food Classifier on PyTorch Mobile</a>
              
            </h5>
            <h6 class="card-subtitle">Nidhin Pattaniyil, Reshama Shaikh</h6>
            <p class="card-text">As technology improves, so does the use of training deep learning models. Additionally, since the time spent on mobile devices is greater than on desktop, the demand for applications running natively on mobile devices is also high. This demo will go through a complete example of training a deep learning vision classifier on the Food-101 dataset using PyTorch. We then deploy it on web and mobile using TorchServe and PyTorch Mobile.</p>
            
            <p class="card-text">
              <a href="https://github.com/npatta01/pytorch-food">https://github.com/npatta01/pytorch-food</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">PERFORMANCE, PRODUCTION & DEPLOYMENT</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D2-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D2.png">Torch-TensorRT: Accelerating Inference Performance Directly from PyTorch using TensorRT</a>
              
            </h5>
            <h6 class="card-subtitle">Naren Dasan, Nick Comly, Dheeraj Peri, Anurag Dixit, Abhiram Iyer, Bo Wang, Arvind Sridhar, Boris Fomitchev, Josh Park</h6>
            <p class="card-text">Learn how to accelerate PyTorch inference, from framework, for model deployment. The PyTorch integration for TensorRT makes the performance of TensorRT's GPU optimizations available in PyTorch for any model. We will walk you through how with 3 lines of code you can go from a trained model to optimized TensorRT-embedded TorchScript, ready to deploy to a production environment.</p>
            
            <p class="card-text">
              <a href="https://github.com/NVIDIA/Torch-TensorRT/">https://github.com/NVIDIA/Torch-TensorRT/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">PERFORMANCE, PRODUCTION & DEPLOYMENT</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D1-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/D1.png">Tensorized Deep Learning with TensorLy-Torch</a>
              
            </h5>
            <h6 class="card-subtitle">Jean Kossaifi</h6>
            <p class="card-text">Most of the data in modern machine learning (e.g. fMRI, videos, etc) is inherently multi-dimensional and leveraging that structure is crucial for good performance. Tensor methods are the natural way to achieve this and can improve deep learning and enable i) large compression ratios through a reduction of the number of parameters, ii) computational speedups, iii) improved performance and iv) better robustness. The TensorLy project provides the tools to manipulate tensors, including tensor algebra, regression and decomposition. TensorLy-Torch builds on top of this and enables tensor-based deep learning by providing out-of-the-box tensor based PyTorch layers that can be readily combined with any deep neural network architecture and takes care of things such as initialization and tensor dropout.</p>
            
            <p class="card-text">
              <a href="http://tensorly.org/quantum">http://tensorly.org/quantum</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">PERFORMANCE, PRODUCTION & DEPLOYMENT</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C12.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C12-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C12.png">Catalyst-Accelerated Deep Learning R&D</a>
              
            </h5>
            <h6 class="card-subtitle">Sergey Kolesnikov</h6>
            <p class="card-text">Catalyst is a PyTorch framework for Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop.</p>
            
            <p class="card-text">
              <a href="https://catalyst-team.com/">https://catalyst-team.com/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C11.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C11-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C11.png">Ray Lightning: Easy Multi-node PyTorch Lightning training</a>
              
            </h5>
            <h6 class="card-subtitle">Amog Kamsetty, Richard Liaw, Will Drevo, Michael Galarnyk</h6>
            <p class="card-text">PyTorch Lightning is a library that provides a high-level interface for PyTorch which helps you organize your code and reduce boilerplate. By abstracting away engineering code, it makes deep learning experiments easier to reproduce and improves developer productivity. PyTorch Lightning also includes plugins to easily parallelize your training across multiple GPUs. This parallel training, however, depends on a critical assumption: that you already have your GPU(s) set up and networked together in an efficient way for training. While you may have a managed cluster like SLURM for multi-node training on the cloud, setting up the cluster and its configuration is no easy task. Ray Lightning was created with this problem in mind to make it easy to leverage multi-node training without needing extensive infrastructure expertise. It is a simple and free plugin for PyTorch Lightning with a number of benefits like simple setup, easy scale up, seamless creation of multi-node clusters on AWS/Azure/GCP via the Ray Cluster Launcher, and an integration with Ray Tune for large-scale distributed hyperparameter search and state of the art algorithms</p>
            
            <p class="card-text">
              <a href="https://github.com/ray-project/ray_lightning">https://github.com/ray-project/ray_lightning</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C10.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C10-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C10.png">Supercharge your Federated Learning with Synergos</a>
              
            </h5>
            <h6 class="card-subtitle">Jin Howe Teo, Way Yen Chen, Najib Ninaba, Choo Heng Chong Mark</h6>
            <p class="card-text">Data sits as the centerpiece of any machine learning endeavour, yet in many real-world projects, a single party’s data is often insufficient and needs to be augmented with data from other sources. This is unfortunately easier said than done, as there are many innate concerns (be it regulatory, ethical, commercial etc.) stopping parties from exchanging data. Fortunately, there exists an emerging privacy-preserving machine learning technology called Federated Learning. It enables multiple parties holding local data to collaboratively train machine learning models without actually exchanging their data with one another, hence preserving the confidentiality of different parties’ local data.Today, we will be showcasing Synergos, a distributed platform built here at AI Singapore to facilitate the adoption of Federated Learning. Specifically, it strives to make the complex mechanisms involved in any federated endeavour simple, accessible and sustainable.</p>
            
            <p class="card-text">
              <a href="https://github.com/aimakerspace/synergos_simulator">https://github.com/aimakerspace/synergos_simulator</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C9.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C9-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C9.png">AdaptDL: An Open-Source Resource-Adaptive Deep Learning Training and Scheduling Framework</a>
              
            </h5>
            <h6 class="card-subtitle">Aurick Qiao, Omkar Pangarkar, Richard Fan</h6>
            <p class="card-text">AdaptDL is an open source framework and scheduling algorithm that directly optimizes cluster-wide training performance and resource utilization. By elastically re-scaling jobs, co-adapting batch sizes and learning rates, and avoiding network interference, AdaptDL improves shared-cluster training compared with alternative schedulers. AdaptDL can automatically determine the optimal number of resources given a job’s need. It will efficiently add or remove resources dynamically to ensure the highest-level performance. The AdaptDL scheduler will automatically figure out the most efficient number of GPUs to allocate to your job, based on its scalability. When the cluster load is low, your job can dynamically expand to take advantage of more GPUs. AdaptDL offers an easy-to-use API to make existing PyTorch training code elastic with adaptive batch sizes and learning rates. We have also ported AdaptDL to Ray/Tune which can automatically scale trials of an Experiment and can be used to schedule stand-alone PyTorch training jobs on the cloud in a cost-effective way.</p>
            
            <p class="card-text">
              <a href="https://github.com/petuum/adaptdl">https://github.com/petuum/adaptdl</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C8-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C8.png">Define-by-run quantization</a>
              
            </h5>
            <h6 class="card-subtitle">Vasiliy Kuznetsov, James Reed, Jerry Zhang</h6>
            <p class="card-text">Describes a prototype PyTorch workflow to perform quantization syntax transforms in Eager mode with: * no model changes needed (compared to Eager mode which requires manual quant/dequant insertion and fusion) * almost no model syntax restrictions (compared to FX graph mode which requires symbolic traceability)</p>
            
            <p class="card-text">
              <a href="https://pytorch.org/docs/stable/quantization.html">https://pytorch.org/docs/stable/quantization.html</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C7.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C7-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C7.png">Fx Numeric Suite Core APIs</a>
              
            </h5>
            <h6 class="card-subtitle">Charles Hernandez, Vasiliy Kuznetzov, Haixin Liu</h6>
            <p class="card-text">wrong when it doesn't satisfy the accuracy we expect. Debugging the accuracy issue of quantization is not easy and time consuming. The Fx Numeric Suite Core APIs allows users to better diagnose the source of their quantization error for both statically and dynamically quantized modelsThis poster gives an overview of the core APIs and techniques available to users through the Fx Numeric Suite, and how they can use them to improve quantization performance.</p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C6-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C6.png">snnTorch: Training spiking neural networks using gradient-based optimization</a>
              
            </h5>
            <h6 class="card-subtitle">J.K. Eshraghian, M. Ward, E.O. Neftci, G. Lenz, X. Wang, G. Dwivedi, M. Bennamoun, D.S. Jeong, W.D. Lu</h6>
            <p class="card-text">The brain is the perfect place to look for inspiration to develop more efficient neural networks. One of the main differences with modern deep learning is that the brain encodes and processes information as spikes rather than continuous activations. Combining the training methods intended for neural networks with the sparse, spiking activity inspired by biological neurons has shown the potential to improve the power efficiency of training and inference by several orders of magnitude. snnTorch is a Python package for performing gradient-based learning with spiking neural networks. It extends the capabilities of PyTorch, taking advantage of its GPU accelerated tensor computation and applying it to networks of event-driven spiking neurons.  snnTorch is designed to be intuitively used with PyTorch, as though each spiking neuron were simply another activation in a sequence of layers. It is therefore agnostic to fully-connected layers, convolutional layers, residual connections, etc. The classical challenges that have faced the neuromorphic engineering community, such as the non-differentiability of spikes, the dead neuron problem, vanishing gradients in backpropagation-through-time, are effectively solved in snnTorch and enable the user to focus on building applications that leverage sparsity and event-driven data streams.</p>
            
            <p class="card-text">
              <a href="https://snntorch.readthedocs.io/en/latest/">https://snntorch.readthedocs.io/en/latest/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C5-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C5.png">PyTorch for R</a>
              
            </h5>
            <h6 class="card-subtitle">Daniel Falbel</h6>
            <p class="card-text">Last year the PyTorch for the R language project has been released allowing R users to benefit of PyTorch's speed and flexibility. Since then we have a growing community of contributors that are both improving the torch for R interface, building research and products on top of it and using it to teach deep learning methods. In this poster we will showcase what are the past and current developments in the PyTorch for R project as well as what are our plans for the future.</p>
            
            <p class="card-text">
              <a href="https://torch.mlverse.org/">https://torch.mlverse.org/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C4-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C4.png">ocaml-torch and tch-rs: writing and using PyTorch models using OCaml or Rust</a>
              
            </h5>
            <h6 class="card-subtitle">Laurent Mazare</h6>
            <p class="card-text">The main front-end for using PyTorch is its Python API, however LibTorch provides a lower level C++ API to manipulate tensors, perform automatic differentiation, etc. ocaml-torch and tch-rs are two open-source projects providing wrappers for this C++ API respectively in OCaml and Rust. Users can then write OCaml and Rust code to create new models, perform inference and training, and benefit from the guarantees provided by strongly typed programming languages and functional programming. They can also use TorchScript to leverage existing Python models. The libraries provide various examples, ranging from the main computer vision models to a minimalist GPT implementation. The main challenges for these bindings are to provide idiomatic APIs adapted to the languages specificities; to automatically generate most of the bindings code as there are thousands of C++ functions to expose; and to interact properly with the memory models for each language.</p>
            
            <p class="card-text">
              <a href="https://github.com/laurentMazare/ocaml-torch">https://github.com/laurentMazare/ocaml-torch</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C3-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C3.png">PyTorch Lightning Flash - Your PyTorch AI Factory</a>
              
            </h5>
            <h6 class="card-subtitle">Ari Bornstein</h6>
            <p class="card-text">Flash is a high-level deep learning framework for fast prototyping, baselining, finetuning and solving deep learning problems. It features a set of tasks for you to use for inference and finetuning out of the box, and an easy to implement API to customize every step of the process for full flexibility. Flash is built for beginners with a simple API that requires very little deep learning background, and for data scientists, Kagglers, applied ML practitioners and deep learning researchers that want a quick way to get a deep learning baseline with advanced features PyTorch Lightning offers. Flash enables you to easily configure and run complex AI recipes for over 15 tasks across 7 data domains</p>
            
            <p class="card-text">
              <a href="https://github.com/PyTorchLightning">https://github.com/PyTorchLightning</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C2-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C2.png">PyTorch-Ignite: Training and evaluating neural networks flexibly and transparently</a>
              
            </h5>
            <h6 class="card-subtitle">Victor Fomin, Taras Savchyn, Priyansi</h6>
            <p class="card-text">PyTorch-Ignite is a high-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently. PyTorch-Ignite is designed to be at the crossroads of high-level Plug & Play features and under-the-hood expansion possibilities. The tool aims to improve the deep learning community's technical skills by promoting best practices where things are not hidden behind a divine tool that does everything, but remain within the reach of users. PyTorch-Ignite differs from other similar tools by allowing users to compose their applications without being focused on a super multi-purpose object, but rather on weakly coupled components allowing advanced customization.</p>
            
            <p class="card-text">
              <a href="https://pytorch-ignite.ai/ecosystem/">https://pytorch-ignite.ai/ecosystem/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C1-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C1.png">Benchmarking the Accuracy and Robustness of Feedback Alignment Methods</a>
              
            </h5>
            <h6 class="card-subtitle">Albert Jimenez, Mohamed Akrout</h6>
            <p class="card-text">Backpropagation is the default algorithm for training deep neural networks due to its simplicity, efficiency and high convergence rate. However, its requirements make it impossible to be implemented in a human brain. In recent years, more biologically plausible learning methods have been proposed. Some of these methods can match backpropagation accuracy, and simultaneously provide other extra benefits such as faster training on specialized hardware (e.g., ASICs) or higher robustness against adversarial attacks. While the interest in the field is growing, there is a necessity for open-source libraries and toolkits to foster research and benchmark algorithms. In this poster, we present BioTorch, a software framework to create, train, and benchmark biologically motivated neural networks. In addition, we investigate the performance of several feedback alignment methods proposed in the literature, thereby unveiling the importance of the forward and backward weight initialization and optimizer choice. Finally, we provide a novel robustness study of these methods against state-of-the-art white and black-box adversarial attacks.</p>
            
            <p class="card-text">
              <a href="https://github.com/jsalbert/biotorch">https://github.com/jsalbert/biotorch</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">EXTENDING PYTORCH, APIs, PARALLEL & DISTRIBUTED TRAINING</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B7.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B7-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B7.png">Salina: Easy programming of Sequential Decision Learning and Reinforcement Learning Models in pytorch</a>
              
            </h5>
            <h6 class="card-subtitle">Ludovic Denoyer, Alfredo de la Fuente, Song Duong, Jean-Baptiste Gaya, Pierre-Alexandre Kamienny, Daniel H. Thompson</h6>
            <p class="card-text">salina is a lightweight library extending PyTorch modules for the development of sequential decision models. It can be used for Reinforcement Learning (including model-based with differentiable environments, multi-agent RL, ...), but also in a supervised/unsupervised learning settings (for instance for NLP, Computer Vision, etc..).</p>
            
            <p class="card-text">
              <a href="https://github.com/facebookresearch/salina">https://github.com/facebookresearch/salina</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ML Ops, MODELS, MODEL OPTIMIZATION & INTERPRETABILITY</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B6-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B6.png">Structured and Unstructured Pruning Workflow in PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Zafar Takhirov, Karen Zhou, Raghuraman Krishnamoorthi</h6>
            <p class="card-text">Two new toolflows for model pruning are introduced: Sparsifier and Pruner, which enable unstructured and structured pruning of the model weights respectively. The toolflow can be combined with other optimization techniques, such as quantization to achieve even higher levels of model compression. In addition to that, the "Pruner" toolflow can also be used for "shape propagation", where the physical structure of the model is modified after structured pruning (in FX graph mode only).This poster gives a high-level overview of the prototype API, usage example, currently supported sparse quantized kernels, as well as provides a brief overview of future plans</p>
            
            <p class="card-text">
              <a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ML Ops, MODELS, MODEL OPTIMIZATION & INTERPRETABILITY</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B5-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B5.png">Torch-CAM: class activation explorer</a>
              
            </h5>
            <h6 class="card-subtitle">François-Guillaume Fernandez</h6>
            <p class="card-text">One of the core inconveniences of Deep Learning comes from its interpretability, which remains obscure for most non-basic convolutional models. Their very      performances are granted by optimization processes that have high degrees of freedom and no constraints on explainability. Fortunately, modern frameworks mechanisms grant access to information flow in their components, which paved the way to building intuition around result interpretability in CNN models. The main contributions of the author are described as follows: - building a flexible framework for class activation computation - providing high-quality implementations of most popular methods - making these methods usable by entry users as well as researchers The open-source project is available here: https://github.com/frgfm/torch-cam</p>
            
            <p class="card-text">
              <a href="https://github.com/frgfm/torch-cam">https://github.com/frgfm/torch-cam</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ML Ops, MODELS, MODEL OPTIMIZATION & INTERPRETABILITY</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B4-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B4.png">moai: A Model Development Kit to Accelerate Data-driven Workflows</a>
              
            </h5>
            <h6 class="card-subtitle">Nikolaos Zioulis</h6>
            <p class="card-text">moai is a PyTorch-based AI Model Development Kit (MDK) that seeks to improve data-driven model workflows, design and understanding. It relies on hydra for handling configuration and lightning for handling infrastructure. As a kit, It offers a set of actions to `train` or `evaluate` models using the corresponding actions which consume configuration files.  Apart from the definition of the model, data, training scheme, optimizer, visualization and logging, these configuration files additionally use named tensors to define tensor processing graphs. These are created by chaining various building blocks called monads, which are functional units or otherwise single responsibility modules. Monad parameters and input/output tensors are defined on the configuration file, allowing for the entire model to be summarized into a single file. This opens up novel functionalities like querying for inter-model differences using the `diff` action, or aggregating the results of multiple models using the `plot` action which uses hiplot to compare models in various ways. moai facilitates high quality reproduction (using the `reprod` action), as apart from automatically handling all boilerplate related to it, it standardizes the process of developing modules/monads and implicitly logs all hyperparameters. Even though no code is required, moai exploits python’s flexibility to allow developers to integrate their own code into its engine from external projects, vastly increasing their productivity.</p>
            
            <p class="card-text">
              <a href="https://github.com/ai-in-motion/moai">https://github.com/ai-in-motion/moai</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ML Ops, MODELS, MODEL OPTIMIZATION & INTERPRETABILITY</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B3-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B3.png">Building Production ML Pipelines for PyTorch Models</a>
              
            </h5>
            <h6 class="card-subtitle">Vaibhav Singh, Rajesh Thallam, Jordan Totten, Karl Weinmeister</h6>
            <p class="card-text">Machine Learning Operationalization has rapidly evolved in the last few years with a growing set of tools for each phase of development. From experimentation to automated model analysis and deployment, each of these tools offer some unique capabilities. In this work we survey a slice of these tools and demonstrate an opinionated example of an end to end CI/CD pipeline for PyTorch model development and deployment using Vertex AI SDK. The goal of this session is to aid an informed conversation on the choices available to PyTorch industry practitioners who are looking to operationalize their ML models, and to researchers who are simply trying to organize their experiments. Although our implementation example will make tool choices at various stages, we will be focused on ML design patterns that are applicable to a wide variety of commercial and open-source offerings.</p>
            
            <p class="card-text">
              <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples">https://github.com/GoogleCloudPlatform/vertex-ai-samples</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ML Ops, MODELS, MODEL OPTIMIZATION & INTERPRETABILITY</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B2-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/B2.png">Customizing MLOps pipelines with JSON-AI: a declarative syntax to streamline ML in the database</a>
              
            </h5>
            <h6 class="card-subtitle">George Hosu, Particio Cerda-Mardini, Natasha Seelam, Jorge Torres</h6>
            <p class="card-text">Nearly 64% of companies take over a month to a year to deploy a single machine learning (ML) model into production [1]. Many of these companies cite key challenges integrating with complex ML frameworks as a root cause [1], as there is still a gap between where data lives, how models are trained, and how downstream applications access predictions from models [1, 2].  MindsDB is a PyTorch-based ML platform that aims to solve fundamental MLOps challenges by abstracting ML models as “virtual tables”, allowing models to be queried in the same natural way users work with data in databases. As data is diverse and varied, we recently developed an open-source declarative syntax, named “JSON-AI” to allow others to customize ML model internals without changing source code.  We believe that the key elements of the data science (DS)/ML pipeline, namely data pre-processing/cleaning, feature engineering, and model-building [2], should be automated in a robust, reliable, and reproducible manner with simplicity. JSON-AI allows you refined control of each of these steps, and enables users to bring custom routines into their ML pipeline. In our poster, we will show how a user interfaces with JSON-AI to bring original approaches to each of the aforementioned parts of the DS/ML, along with control over analysis and explainability tools. [1] Algorithmia (2021). 2021 state of enterprise machine learning [2] “How Much Automation Does a Data Scientist Want?” ArXiV (2021)</p>
            
            <p class="card-text">
              <a href="https://github.com/mindsdb/mindsdb/">https://github.com/mindsdb/mindsdb/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ML Ops, MODELS, MODEL OPTIMIZATION & INTERPRETABILITY</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A10.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A10-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A10.png">TorchStudio, a full featured IDE for PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Robin Lobel</h6>
            <p class="card-text">TorchStudio is an open-source, full-featured IDE for PyTorch. It aims to simplify the creation, training and iterations of AI models. It can load, analyze and explore datasets from the TorchVision or TorchAudio categories, or custom datasets with any format and number of inputs and outputs. TorchVision, TorchAudio or custom models can then be loaded or written from scratch, debugged, visualized as a graph, and trained using local hardware, a distant server or GPUs in the cloud. Trainings can then be compared in the dashboard with several analyzing tools to help you identify the best performing set of models and hyper parameters and export it as TorchScript or ONNX files. TorchStudio is also highly customizable, with 90% of its functionalities accessible as open source scripts and independent modules, to fit as many AI scenario as possible.</p>
            
            <p class="card-text">
              <a href="https://torchstudio.ai/">https://torchstudio.ai/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ACCELERATORS, TOOLS, LIBRARY, DATA</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A9.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A9-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A9.png">Accelerate TorchServe with Intel Extension for PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Mark Saroufim, Hamid Shojanazeri, Patrick Hu, Geeta Chauhan, Jing Xu, Jianan Gu, Jiong Gong, Ashok Emani, Eikan Wang, Min Jean Cho, Fan Zhao</h6>
            <p class="card-text">Accelerate TorchServe with Intel® Extension for PyTorch: Intel is collaborating with Meta to take advantage of performance boosting from Intel® Extension for PyTorch* from TorchServe, so that users can easily deploy their PyTorch models with out of the box satisfying performance. With these SW advancements, we demonstrated ease-of-use IPEX user-facing API, and we also showcased speed-up with Intel® Extension for PyTorch* FP32 inference with the stock PyTorch and speed-up with Intel® Extension for PyTorch* INT8 inference with the stock PyTorch.</p>
            
            <p class="card-text">
              <a href="www.intel.com/Performanceindex">www.intel.com/Performanceindex</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ACCELERATORS, TOOLS, LIBRARY, DATA</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A3-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A3.png">Kaolin Library</a>
              
            </h5>
            <h6 class="card-subtitle">Clement Fuji Tsang, Jean-Francois Lafleche, Charles Loop, Masha Shugrina, Towaki Takikawa, Jiehan Wang</h6>
            <p class="card-text">NVIDIA Kaolin is a suite of tools for accelerating 3D Deep Learning research. The Kaolin library provides a PyTorch API for working with a variety of 3D representations and includes a growing collection of GPU-optimized operations such as modular differentiable rendering, fast conversions between representations, loss functions, data loading, 3D checkpoints and more. The library also contains a lightweight 3D visualizer Dash3D and can work with an Omniverse companion app for dataset/checkpoint visualization and synthetic data generation.</p>
            
            <p class="card-text">
              <small class="text-muted">ACCELERATORS, TOOLS, LIBRARY, DATA</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A2-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A2.png">Accelerate PyTorch training with Cloud TPUs</a>
              
            </h5>
            <h6 class="card-subtitle">Jack Cao, Milad Mohammadi, Zak Stone, Vaibhav Singh, Calvin Pelletier, Shauheen Zahirazami</h6>
            <p class="card-text">PyTorch / XLA offers PyTorch users the ability to train their models on XLA devices including Cloud TPUs. This compiled path often makes it possible to utilize creative optimizations and achieve top performance on target XLA devices. With the introduction of Cloud TPU VMs, users have direct access to TPU host machines and therefore a great level of flexibility. In addition, TPU VMs make debugging easier and reduce data transfer overheads. Google has also recently announced the availability of Cloud TPU v4 Pods, which are exaflop-scale supercomputers for machine learning. Cloud TPU v4 Pods offer a whole new level of performance for large-scale PyTorch / XLA training of ML models.</p>
            
            <p class="card-text">
              <small class="text-muted">ACCELERATORS, TOOLS, LIBRARY, DATA</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A1-thumb.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/A1.png">Accelerating PyTorch on the largest chip ever built (WSE)</a>
              
            </h5>
            <h6 class="card-subtitle">Antonio Kim, Behzad Abghari, Chris Oliver, Cynthia Liu, Mark Browning, Vishal Subbiah, Kamran Jafari, Emad Barsoum, Jessica Liu, Sean Lie</h6>
            <p class="card-text">The Cerebras Wafer Scale Engine (WSE) is the largest processor ever built, dedicated to accelerating deep learning model for training and inference. A single chip in a single CS-2 system provides the compute power of a cluster of GPUs but acts as a single processor, making it also much simpler to use. We present the current PyTorch backend architecture for the Cerebras CS-2 and how we go all the way from PyTorch to laying out the model graph on the wafer. Additionally, we will discuss the advantages of training on Cerebras hardware and its unique capabilities.</p>
            
            <p class="card-text">
              <a href="https://cerebras.net">https://cerebras.net</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">ACCELERATORS, TOOLS, LIBRARY, DATA</small>
            </p>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</div>
<script type="text/javascript">
  // this isn't efficient please don't use for more than a few hundred items
  $("#pted-filter").on("keyup", function () {
    var input = $(this).val().toLowerCase();

    $(".card").filter(function () {
      $(this).toggle($(this).text().toLowerCase().indexOf(input) > -1);
    });
  });
</script>

    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access documentation for PyPose</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get started with tutorials and examples</p>
        <a class="with-right-arrow" href="https://pypose.org/tutorials/">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Get Started</h2>
        <p>Find resources and how to start using PyPose</p>
        <a class="with-right-arrow" href="/get-started">Get Started</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pypose.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pypose.org">PyPose</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <!-- <li><a href="/ecosystem">Ecosystem</a></li> -->
          <!-- <li><a href="/blog">Blog</a></li> -->
          <li><a href="https://github.com/pypose/pypose/blob/main/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pypose.org/tutorials/">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <!-- <li><a href="" target="_blank">Discuss</a></li> -->
          <li><a href="https://github.com/pypose/pypose/issues" target="_blank">GitHub Issues</a></li>
          <!-- <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li> -->
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <!-- <li><a href="" target="_blank">Facebook</a></li> -->
          <li><a href="https://twitter.com/pypose_org" target="_blank">Twitter</a></li>
          <!-- <li><a href="" target="_blank">YouTube</a></li> -->
          <li><a href="https://github.com/pypose/pypose" target="_blank">GitHub</a></li>
        </ul>
      </div>

      <!-- <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="" target="_blank">Spotify</a></li>
          <li><a href="" target="_blank">Apple</a></li>
          <li><a href="" target="_blank">Google</a></li>
          <li><a href="" target="_blank">Amazon</a></li>
        </ul>
      </div> -->
    </div>

    <!-- <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-tos-privacy-policy.pdf" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-oss-privacy-policy.pdf" target="_blank">Privacy</a></li>
      </ul>
    </div> -->
  </div>
  <!-- <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>
 -->
</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pypose.org" aria-label="PyPose"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <!-- <li class="resources-mobile-menu-title">
          <a href="/ecosystem">Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/pted/2021">Ecosystem Day 2021</a>
          </li>
          <li>
            <a href="/ecosystem/ptdd/2021">Developer Day 2021</a>
          </li>
        </ul> -->

        <!-- <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="">
          <a href="/blog">Blog</a>
        </li> -->

        <li>
          <a href="https://pypose.org/tutorials/">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <li class="">
          <a href="/about-us">About Us</a>
        </li>

        <!-- <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/docs/stable/torchvision">torchvision</a>
          </li>

          <li class="">
            <a href="/elastic">TorchElastic</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul> -->

        <!-- <li class="resources-mobile-menu-title">
          Resources
        </li> -->

        <!-- <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/#community-module">Community</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul> -->

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pypose/pypose">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<!-- <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>
 -->

  </body>
</html>
